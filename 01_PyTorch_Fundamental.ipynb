{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f549f86e",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 1: PyTorch Fundamentals**  \n",
    "\n",
    "**Introduction**  \n",
    "\n",
    "PyTorch has emerged as one of the most popular open-source frameworks for deep learning, renowned for its flexibility, dynamic computation graphs, and Python-first philosophy. Developed by Meta (formerly Facebook) and now part of the Linux Foundation, PyTorch powers cutting-edge research and production systems alike—from computer vision to natural language processing.  \n",
    "\n",
    "This chapter lays the groundwork for your PyTorch journey. You’ll start by understanding PyTorch’s core data structure: **tensors**, which are the building blocks of all operations. You’ll learn how to create, manipulate, and optimize tensors, while mastering key concepts like **broadcasting**, **memory management**, and **GPU acceleration**. By the end, you’ll be equipped to:  \n",
    "- Install PyTorch and verify CPU/GPU support.  \n",
    "- Perform tensor operations with NumPy-like syntax.  \n",
    "- Efficiently handle memory and device transfers.  \n",
    "- Apply indexing and broadcasting rules to avoid common pitfalls.  \n",
    "\n",
    "Whether you’re transitioning from NumPy or starting fresh, these fundamentals will serve as the foundation for every neural network you build. Let’s begin!  \n",
    "\n",
    "\n",
    "\n",
    "### **Key Topics Covered**  \n",
    "1. **Introduction to PyTorch**  \n",
    "   - History, use cases, and comparison with TensorFlow.  \n",
    "2. **Installation and Setup**  \n",
    "   - Configuring PyTorch for CPU/GPU environments.  \n",
    "3. **Tensors**  \n",
    "   - Creation methods, math operations, and properties.  \n",
    "4. **Indexing & Slicing**  \n",
    "   - Subsetting tensors (inspired by NumPy).  \n",
    "5. **Broadcasting Rules**  \n",
    "   - How PyTorch handles operations between tensors of different shapes.  \n",
    "6. **Memory Management**  \n",
    "   - Views vs. copies, and GPU-CPU transfers.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9f4be",
   "metadata": {},
   "source": [
    "### **1.1 Introduction to PyTorch**  \n",
    "\n",
    "#### **What is PyTorch?**  \n",
    "PyTorch is an **open-source deep learning framework** developed by Meta (Facebook) and now maintained by the Linux Foundation. It provides a flexible and intuitive platform for building, training, and deploying machine learning models, particularly neural networks.  \n",
    "\n",
    "Unlike static computation graph frameworks (like early TensorFlow), PyTorch uses a **dynamic computation graph (define-by-run)**, meaning the graph is built on-the-fly as operations are executed. This makes debugging easier and allows for more natural Python-like coding.  \n",
    "\n",
    "\n",
    "\n",
    "#### **Key Features of PyTorch**  \n",
    "\n",
    "| Feature | Why It Matters |\n",
    "|---------|---------------|\n",
    "| **Dynamic Computation Graphs** | Enables flexible model architectures (e.g., variable-length RNNs). |\n",
    "| **Pythonic & NumPy-like Syntax** | Easy to learn if you know Python/NumPy. |\n",
    "| **GPU Acceleration (CUDA)** | Speeds up deep learning computations significantly. |\n",
    "| **Autograd (Automatic Differentiation)** | Automatically computes gradients for backpropagation. |\n",
    "| **TorchScript & ONNX Support** | Allows model deployment in production environments. |\n",
    "| **Rich Ecosystem (TorchVision, TorchText, TorchAudio)** | Prebuilt tools for CV, NLP, and audio tasks. |\n",
    "| **Distributed Training** | Scales training across multiple GPUs/machines. |\n",
    "\n",
    "\n",
    "\n",
    "#### **PyTorch vs. TensorFlow**  \n",
    "\n",
    "| Aspect | PyTorch | TensorFlow (2.x+) |\n",
    "|--------|---------|------------------|\n",
    "| **Graph Type** | Dynamic (eager execution by default) | Static (but supports eager execution) |\n",
    "| **Debugging** | Easier (Python-like execution) | More complex (historically) |\n",
    "| **Deployment** | TorchScript, ONNX, Flask | TensorFlow Serving, TFLite |\n",
    "| **Research Popularity** | Dominant in academia | More common in industry |\n",
    "| **Learning Curve** | More intuitive | Steeper initially |\n",
    "\n",
    "**When to Choose PyTorch?**  \n",
    "✔ Research & prototyping  \n",
    "✔ Dynamic architectures (e.g., attention models)  \n",
    "✔ If you prefer Pythonic code  \n",
    "\n",
    "**When to Consider TensorFlow?**  \n",
    "✔ Production pipelines (e.g., Google Cloud TPUs)  \n",
    "✔ Mobile/edge deployment (TFLite)  \n",
    "\n",
    "\n",
    "\n",
    "#### **PyTorch in the Real World**  \n",
    "- **Research**: Used in most AI papers (e.g., GPT, Stable Diffusion).  \n",
    "- **Industry**: Meta, Tesla, and OpenAI rely on PyTorch.  \n",
    "- **Education**: Preferred for teaching due to simplicity.  \n",
    "\n",
    "\n",
    "\n",
    "#### **Code Example: Why PyTorch Feels Like NumPy**  \n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Similar to NumPy, but with GPU support\n",
    "x = torch.rand(3, 3)  # Random tensor\n",
    "y = torch.ones(3, 3)  \n",
    "z = x + y  # Element-wise addition (broadcasting supported)\n",
    "\n",
    "print(z)\n",
    "```\n",
    "**Output**:  \n",
    "```\n",
    "tensor([[1.2314, 1.8421, 1.5732],\n",
    "        [1.9923, 1.4821, 1.3356],\n",
    "        [1.6701, 1.3832, 1.9021]])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c2e94",
   "metadata": {},
   "source": [
    "### **1.2 Installation and Setup (CPU/GPU)**  \n",
    "\n",
    "PyTorch can run on both **CPU and GPU (CUDA)** devices. Setting it up correctly ensures optimal performance, especially for deep learning tasks.  \n",
    "\n",
    "\n",
    "## **Step 1: Install PyTorch**  \n",
    "\n",
    "### **Official Installation (via PyTorch.org)**  \n",
    "The easiest way is to use the [official PyTorch installer](https://pytorch.org/get-started/locally/):  \n",
    "\n",
    "1. Go to **https://pytorch.org/get-started/locally/**  \n",
    "2. Select:  \n",
    "   - **OS** (Windows, Linux, macOS)  \n",
    "   - **Package Manager** (`pip` or `conda`)  \n",
    "   - **Python Version**  \n",
    "   - **CUDA Version** (if using GPU)  \n",
    "\n",
    "Example:  \n",
    "```bash\n",
    "# For CUDA 12.1 (latest stable GPU support)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# For CPU-only (no GPU)\n",
    "pip install torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "### **Alternative: Conda (Anaconda/Miniconda)**\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **Step 2: Verify Installation**  \n",
    "Check if PyTorch is installed correctly:  \n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Check PyTorch version\n",
    "print(torch.__version__)  # e.g., \"2.3.0\"\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "print(torch.cuda.is_available())  # True if GPU is detected\n",
    "```\n",
    "\n",
    "✅ **Expected Output:**  \n",
    "```\n",
    "2.3.0  \n",
    "True  # (If GPU is available)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: GPU Setup (NVIDIA CUDA & cuDNN)**  \n",
    "\n",
    "### **Requirements for GPU Support**  \n",
    "- **NVIDIA GPU** (Check compatibility [here](https://developer.nvidia.com/cuda-gpus))  \n",
    "- **CUDA Toolkit** (Installs GPU drivers)  \n",
    "- **cuDNN** (Optimized deep learning library)  \n",
    "\n",
    "### **Install CUDA & cuDNN**  \n",
    "1. **Install CUDA Toolkit**  \n",
    "   - Download from [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)  \n",
    "   - Example (Linux):  \n",
    "     ```bash\n",
    "     wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "     sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "     sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub\n",
    "     sudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /\"\n",
    "     sudo apt-get update\n",
    "     sudo apt-get -y install cuda-12.1\n",
    "     ```\n",
    "2. **Install cuDNN**  \n",
    "   - Download from [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) (requires NVIDIA account)  \n",
    "   - Extract and copy files to CUDA directory:  \n",
    "     ```bash\n",
    "     tar -xzvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz\n",
    "     sudo cp cuda/include/* /usr/local/cuda/include/\n",
    "     sudo cp cuda/lib64/* /usr/local/cuda/lib64/\n",
    "     ```\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Troubleshooting Common Issues**  \n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| **`torch.cuda.is_available()` returns `False`** | 1. Check GPU compatibility <br> 2. Reinstall CUDA/cuDNN <br> 3. Update NVIDIA drivers |\n",
    "| **\"CUDA out of memory\"** | Reduce batch size or use `torch.cuda.empty_cache()` |\n",
    "| **Slow performance on GPU** | Ensure `tensor.to(device)` is used correctly |\n",
    "| **DLL load failed (Windows)** | Install Microsoft Visual C++ Redistributable |\n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Switching Between CPU & GPU**  \n",
    "PyTorch allows seamless switching between devices:  \n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move tensors/models to GPU\n",
    "x = torch.rand(3, 3).to(device)\n",
    "model = MyModel().to(device)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **Final Check: GPU Benchmarking**  \n",
    "Test if GPU acceleration works:  \n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "# CPU test\n",
    "start = time.time()\n",
    "a = torch.rand(10000, 10000)\n",
    "b = torch.rand(10000, 10000)\n",
    "c = a @ b  # Matrix multiplication\n",
    "print(f\"CPU Time: {time.time() - start:.2f}s\")\n",
    "\n",
    "# GPU test (if available)\n",
    "if torch.cuda.is_available():\n",
    "    start = time.time()\n",
    "    a = a.to(\"cuda\")\n",
    "    b = b.to(\"cuda\")\n",
    "    c = a @ b\n",
    "    print(f\"GPU Time: {time.time() - start:.2f}s\")\n",
    "```\n",
    "\n",
    "✅ **Expected Output:**  \n",
    "```\n",
    "CPU Time: 3.45s  \n",
    "GPU Time: 0.12s  # (Much faster!)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dc9b3",
   "metadata": {},
   "source": [
    "### **1.2 Installation and Setup (CPU/GPU)**  \n",
    "\n",
    "PyTorch can run on both **CPU and GPU (CUDA)** devices. Setting it up correctly ensures optimal performance, especially for deep learning tasks.  \n",
    "\n",
    "\n",
    "## **Step 1: Install PyTorch**  \n",
    "\n",
    "### **Official Installation (via PyTorch.org)**  \n",
    "The easiest way is to use the [official PyTorch installer](https://pytorch.org/get-started/locally/):  \n",
    "\n",
    "1. Go to **https://pytorch.org/get-started/locally/**  \n",
    "2. Select:  \n",
    "   - **OS** (Windows, Linux, macOS)  \n",
    "   - **Package Manager** (`pip` or `conda`)  \n",
    "   - **Python Version**  \n",
    "   - **CUDA Version** (if using GPU)  \n",
    "\n",
    "Example:  \n",
    "```bash\n",
    "# For CUDA 12.1 (latest stable GPU support)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# For CPU-only (no GPU)\n",
    "pip install torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "### **Alternative: Conda (Anaconda/Miniconda)**\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **Step 2: Verify Installation**  \n",
    "Check if PyTorch is installed correctly:  \n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Check PyTorch version\n",
    "print(torch.__version__)  # e.g., \"2.3.0\"\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "print(torch.cuda.is_available())  # True if GPU is detected\n",
    "```\n",
    "\n",
    "✅ **Expected Output:**  \n",
    "```\n",
    "2.3.0  \n",
    "True  # (If GPU is available)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: GPU Setup (NVIDIA CUDA & cuDNN)**  \n",
    "\n",
    "### **Requirements for GPU Support**  \n",
    "- **NVIDIA GPU** (Check compatibility [here](https://developer.nvidia.com/cuda-gpus))  \n",
    "- **CUDA Toolkit** (Installs GPU drivers)  \n",
    "- **cuDNN** (Optimized deep learning library)  \n",
    "\n",
    "### **Install CUDA & cuDNN**  \n",
    "1. **Install CUDA Toolkit**  \n",
    "   - Download from [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)  \n",
    "   - Example (Linux):  \n",
    "     ```bash\n",
    "     wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "     sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "     sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub\n",
    "     sudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/ /\"\n",
    "     sudo apt-get update\n",
    "     sudo apt-get -y install cuda-12.1\n",
    "     ```\n",
    "2. **Install cuDNN**  \n",
    "   - Download from [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) (requires NVIDIA account)  \n",
    "   - Extract and copy files to CUDA directory:  \n",
    "     ```bash\n",
    "     tar -xzvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz\n",
    "     sudo cp cuda/include/* /usr/local/cuda/include/\n",
    "     sudo cp cuda/lib64/* /usr/local/cuda/lib64/\n",
    "     ```\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Troubleshooting Common Issues**  \n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| **`torch.cuda.is_available()` returns `False`** | 1. Check GPU compatibility <br> 2. Reinstall CUDA/cuDNN <br> 3. Update NVIDIA drivers |\n",
    "| **\"CUDA out of memory\"** | Reduce batch size or use `torch.cuda.empty_cache()` |\n",
    "| **Slow performance on GPU** | Ensure `tensor.to(device)` is used correctly |\n",
    "| **DLL load failed (Windows)** | Install Microsoft Visual C++ Redistributable |\n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Switching Between CPU & GPU**  \n",
    "PyTorch allows seamless switching between devices:  \n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move tensors/models to GPU\n",
    "x = torch.rand(3, 3).to(device)\n",
    "model = MyModel().to(device)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **Final Check: GPU Benchmarking**  \n",
    "Test if GPU acceleration works:  \n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "# CPU test\n",
    "start = time.time()\n",
    "a = torch.rand(10000, 10000)\n",
    "b = torch.rand(10000, 10000)\n",
    "c = a @ b  # Matrix multiplication\n",
    "print(f\"CPU Time: {time.time() - start:.2f}s\")\n",
    "\n",
    "# GPU test (if available)\n",
    "if torch.cuda.is_available():\n",
    "    start = time.time()\n",
    "    a = a.to(\"cuda\")\n",
    "    b = b.to(\"cuda\")\n",
    "    c = a @ b\n",
    "    print(f\"GPU Time: {time.time() - start:.2f}s\")\n",
    "```\n",
    "\n",
    "✅ **Expected Output:**  \n",
    "```\n",
    "CPU Time: 3.45s  \n",
    "GPU Time: 0.12s  # (Much faster!)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81926164",
   "metadata": {},
   "source": [
    "### **1.4 Tensor Indexing and Slicing**  \n",
    "*(Advanced techniques for accessing and modifying tensor elements)*  \n",
    "\n",
    "PyTorch's indexing syntax is heavily inspired by **NumPy**, offering powerful ways to extract, modify, and manipulate tensor data. Here's a complete breakdown:\n",
    "\n",
    "\n",
    "\n",
    "## **1. Basic Indexing (Single Elements)**\n",
    "```python\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "# Get single element (row 1, column 2)\n",
    "print(tensor[1, 2])  # Output: 6\n",
    "\n",
    "# Modify element\n",
    "tensor[0, 1] = 99    # Now: [[1, 99, 3], ...]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **2. Slicing (Sub-Tensors)**\n",
    "| Syntax | Description | Example |\n",
    "|--------|-------------|---------|\n",
    "| `tensor[start:end]` | Range of indices | `tensor[0:2, 1:]` |\n",
    "| `tensor[start:end:step]` | With step size | `tensor[::2, ::-1]` |\n",
    "| `tensor[...]` | Ellipsis (auto-complete dims) | `tensor[..., 0]` |\n",
    "\n",
    "**Examples:**\n",
    "```python\n",
    "x = torch.arange(9).view(3, 3)  # 3x3 tensor\n",
    "\n",
    "# Get first 2 rows, last 2 columns\n",
    "print(x[:2, -2:])  # tensor([[1, 2], [4, 5]])\n",
    "\n",
    "# Reverse every other row\n",
    "print(x[::2, :])   # tensor([[0, 1, 2], [6, 7, 8]])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **3. Advanced Indexing Techniques**\n",
    "\n",
    "### **A. Boolean Masking**\n",
    "```python\n",
    "mask = x > 4\n",
    "print(x[mask])  # tensor([5, 6, 7, 8]) - Flattened result\n",
    "\n",
    "# Modify masked elements\n",
    "x[x > 4] = 0    # Sets all >4 values to 0\n",
    "```\n",
    "\n",
    "### **B. Integer Array Indexing**\n",
    "```python\n",
    "rows = torch.tensor([0, 2])\n",
    "cols = torch.tensor([1, 0])\n",
    "print(x[rows, cols])  # tensor([1, 6]) - (x[0,1], x[2,0])\n",
    "```\n",
    "\n",
    "### **C. Combined Indexing**\n",
    "```python\n",
    "# First column where rows > 1\n",
    "print(x[x[:, 0] > 1, 0])  # tensor([3, 6])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **4. Special Cases**\n",
    "\n",
    "### **A. `torch.where()` (Conditional Selection)**\n",
    "```python\n",
    "y = torch.where(x > 2, x, -1)  # Replace values <=2 with -1\n",
    "```\n",
    "\n",
    "### **B. `torch.masked_select()`**\n",
    "```python\n",
    "selected = torch.masked_select(x, x > 2)  # Returns 1D tensor\n",
    "```\n",
    "\n",
    "### **C. `torch.take()` (Flattened Indexing)**\n",
    "```python\n",
    "indices = torch.tensor([2, 5])\n",
    "print(x.take(indices))  # tensor([2, 5]) - Treats x as 1D\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **5. In-Place Modification**\n",
    "Use `_` suffix to modify tensors directly:\n",
    "```python\n",
    "x[1:, 1:] += 10  # Adds 10 to bottom-right 2x2 submatrix\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **6. Dimension-Specific Operations**\n",
    "| Operation | Syntax | Example |\n",
    "|-----------|--------|---------|\n",
    "| **Select along dim** | `torch.select()` | `x.select(1, 2)` (3rd column) |\n",
    "| **Narrow** | `torch.narrow()` | `x.narrow(0, 1, 2)` (rows 1-2) |\n",
    "| **Index fill** | `.index_fill_()` | `x.index_fill_(0, idx, value)` |\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Fill specific indices with value\n",
    "x.index_fill_(1, torch.tensor([0, 2]), -1)  # Columns 0 & 2 set to -1\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **7. Practical Applications**\n",
    "1. **Cropping Images**:\n",
    "   ```python\n",
    "   image_tensor = torch.rand(3, 256, 256)  # (C, H, W)\n",
    "   cropped = image_tensor[:, 50:200, 30:-30]\n",
    "   ```\n",
    "\n",
    "2. **Batch Processing**:\n",
    "   ```python\n",
    "   batch = torch.rand(32, 10)  # (batch_size, features)\n",
    "   first_5_samples = batch[:5] \n",
    "   ```\n",
    "\n",
    "3. **Attention Masks (NLP)**:\n",
    "   ```python\n",
    "   seq_len = 50\n",
    "   mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "## **Common Pitfalls**\n",
    "1. **Clone Before Modifying Slices**:\n",
    "   ```python\n",
    "   slice = x[0:2].clone()  # Avoids modifying original\n",
    "   ```\n",
    "\n",
    "2. **Avoid Mixed Index Types**:\n",
    "   ```python\n",
    "   # Bad: x[0, [1, 2]] \n",
    "   # Good: x[[0, 0], [1, 2]]  # Explicit indices\n",
    "   ```\n",
    "\n",
    "3. **Non-Contiguous Slices**:\n",
    "   ```python\n",
    "   y = x.t()  # Transpose makes non-contiguous\n",
    "   z = y.contiguous()  # Ensures memory layout\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "## **Cheatsheet Summary**\n",
    "| Task | Syntax |\n",
    "|------|--------|\n",
    "| Get single element | `tensor[i,j]` |\n",
    "| Slice submatrix | `tensor[start:end:step, dim]` |\n",
    "| Conditional select | `tensor[mask]` or `torch.where()` |\n",
    "| Fancy indexing | `tensor[[row_idx], [col_idx]]` |\n",
    "| Modify in-place | `tensor[idx] = value` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013fc47",
   "metadata": {},
   "source": [
    "### **1.5 Broadcasting Rules in PyTorch**  \n",
    "*(How PyTorch automatically expands tensors for operations with mismatched shapes)*  \n",
    "\n",
    "\n",
    "\n",
    "## **1. What is Broadcasting?**  \n",
    "Broadcasting is PyTorch's mechanism to perform **element-wise operations** on tensors with **different shapes** by automatically expanding the smaller tensor to match the larger one **without copying data**.  \n",
    "\n",
    "**Key Idea**:  \n",
    "- Operations like `+`, `-`, `*`, `/` support broadcasting.  \n",
    "- Works from **right to left** (trailing dimensions).  \n",
    "\n",
    "\n",
    "\n",
    "## **2. Broadcasting Rules**  \n",
    "Two tensors are \"broadcastable\" if:  \n",
    "1. **Rule 1**: Trailing dimensions must match **or be 1**.  \n",
    "2. **Rule 2**: Missing dimensions are treated as size **1**.  \n",
    "\n",
    "PyTorch compares shapes **dimension-wise** starting from the right:  \n",
    "- If dimensions are **equal**, proceed.  \n",
    "- If one dimension is **1**, stretch it to match.  \n",
    "- If **dimension is missing**, pretend it’s **1**.  \n",
    "\n",
    "\n",
    "\n",
    "## **3. Step-by-Step Examples**  \n",
    "\n",
    "### **Example 1: Vector + Scalar**  \n",
    "```python\n",
    "a = torch.tensor([1, 2, 3])  # Shape: (3)\n",
    "b = 5                        # Shape: () → (1) → (3) via broadcasting\n",
    "\n",
    "result = a + b  # tensor([6, 7, 8])\n",
    "```\n",
    "**Steps**:  \n",
    "1. `b` (scalar) is treated as `torch.tensor(5, shape=(1,))`.  \n",
    "2. `b` is stretched to `(3,)` to match `a`.  \n",
    "\n",
    "\n",
    "\n",
    "### **Example 2: Matrix + Vector (Common Case)**  \n",
    "```python\n",
    "a = torch.rand(3, 4)  # Shape: (3, 4)\n",
    "b = torch.rand(4)      # Shape: (4) → (1, 4) → (3, 4)\n",
    "\n",
    "result = a + b  # Valid\n",
    "```\n",
    "**Steps**:  \n",
    "1. `b`’s shape becomes `(1, 4)` (Rule 2).  \n",
    "2. `b` is stretched along dim=0 to `(3, 4)`.  \n",
    "\n",
    "\n",
    "\n",
    "### **Example 3: Incompatible Shapes**  \n",
    "```python\n",
    "a = torch.rand(3, 4, 5)\n",
    "b = torch.rand(3, 5)    # Shape mismatch at dim=1 (4 vs 5)\n",
    "\n",
    "# This will ERROR:\n",
    "# result = a + b  \n",
    "```\n",
    "**Why?**:  \n",
    "- Dim 1: `4` (a) != `5` (b) and neither is 1 → **Not broadcastable**.  \n",
    "\n",
    "\n",
    "\n",
    "## **4. Explicit Expansion with `expand()` and `unsqueeze()`**  \n",
    "If broadcasting fails, manually reshape tensors:  \n",
    "\n",
    "### **A. `unsqueeze()`: Add Singleton Dimensions**  \n",
    "```python\n",
    "a = torch.rand(3, 4)\n",
    "b = torch.rand(4)       # Shape: (4)\n",
    "\n",
    "# Manually match shapes\n",
    "b = b.unsqueeze(0)     # Shape: (1, 4)\n",
    "result = a + b.expand(3, 4)  # Explicit expansion\n",
    "```\n",
    "\n",
    "### **B. `expand()`: Repeat Data (No Copy)**  \n",
    "```python\n",
    "b = torch.tensor([1, 2, 3])  # Shape: (3)\n",
    "b_expanded = b.expand(2, 3)   # Shape: (2, 3)  \n",
    "# Repeats rows: [[1, 2, 3], [1, 2, 3]]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **5. Common Use Cases**  \n",
    "\n",
    "### **A. Normalizing a Batch of Images**  \n",
    "```python\n",
    "images = torch.rand(32, 3, 256, 256)  # (N, C, H, W)\n",
    "mean = torch.tensor([0.5, 0.5, 0.5])  # Shape: (3)\n",
    "\n",
    "# Broadcasting: mean becomes (1, 3, 1, 1)\n",
    "normalized = images - mean.view(1, 3, 1, 1)  \n",
    "```\n",
    "\n",
    "### **B. Adding Bias to Linear Layer Output**  \n",
    "```python\n",
    "output = torch.rand(10, 50)  # (batch_size, features)\n",
    "bias = torch.rand(50)        # Shape: (50)\n",
    "\n",
    "# Broadcasting: bias becomes (1, 50) → (10, 50)\n",
    "output += bias  \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **6. Debugging Broadcasting Errors**  \n",
    "**Error Message**:  \n",
    "`RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at dimension 1`  \n",
    "\n",
    "**Solution**:  \n",
    "1. Check shapes with `.shape`.  \n",
    "2. Use `.unsqueeze()` or `.reshape()` to align dimensions.  \n",
    "3. Verify trailing dimensions match (or are 1).  \n",
    "\n",
    "**Example Fix**:  \n",
    "```python\n",
    "a = torch.rand(3, 4)\n",
    "b = torch.rand(3, 5)\n",
    "\n",
    "# Option 1: Reshape b to be broadcastable\n",
    "b = b[:, :4]  # Slice to match dim=1\n",
    "\n",
    "# Option 2: Expand via unsqueeze + expand\n",
    "b = b.unsqueeze(1).expand(3, 4, 5)  # New shape: (3, 4, 5)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **7. Broadcasting Cheatsheet**  \n",
    "\n",
    "| Scenario | How to Fix |\n",
    "|----------|------------|\n",
    "| **Scalar + Tensor** | Automatic (e.g., `tensor + 5`) |  \n",
    "| **Vector + Matrix** | Ensure vector is `(..., 1)` or `(1, ...)` |  \n",
    "| **Mismatched trailing dims** | Slice or pad tensors |  \n",
    "| **Missing dimensions** | Use `unsqueeze()` |  \n",
    "\n",
    "**Key Functions**:  \n",
    "- `tensor.unsqueeze(dim)` - Add dimension at `dim`.  \n",
    "- `tensor.expand(new_shape)` - Repeat data to match shape.  \n",
    "- `tensor.view()` or `reshape()` - Align dimensions.  \n",
    "\n",
    "\n",
    "\n",
    "## **8. Performance Considerations**  \n",
    "- Broadcasting is **memory-efficient** (no data copied).  \n",
    "- But **overuse** can lead to confusion. Prefer explicit shapes when possible. \n",
    "\n",
    "\n",
    "#### **visual diagram** \n",
    "\n",
    "\n",
    "### **Broadcasting Visualization**  \n",
    "*(PyTorch expands tensors from right to left)*  \n",
    "\n",
    "#### **Example 1: Vector + Scalar**  \n",
    "```python\n",
    "Tensor A: [1, 2, 3]  # Shape (3)\n",
    "Scalar B: 5           # Shape () → (1) → (3)\n",
    "```\n",
    "**Step-by-Step Expansion**:  \n",
    "```\n",
    "A: [1, 2, 3]  \n",
    "B: 5 → [5, 5, 5]  # Stretched to match A's shape\n",
    "Result: [1+5, 2+5, 3+5] = [6, 7, 8]\n",
    "```\n",
    "\n",
    "#### **Example 2: Matrix + Vector**  \n",
    "```python\n",
    "Tensor A: [[1, 2, 3],   # Shape (2, 3)\n",
    "           [4, 5, 6]]  \n",
    "Tensor B: [10, 20, 30]   # Shape (3) → (1, 3) → (2, 3)\n",
    "```\n",
    "**Expansion Process**:  \n",
    "```\n",
    "B: [10, 20, 30]  \n",
    "   → [[10, 20, 30],     # Dim 0 stretched\n",
    "      [10, 20, 30]]  \n",
    "Result: [[1+10, 2+20, 3+30],  \n",
    "         [4+10, 5+20, 6+30]]\n",
    "```\n",
    "\n",
    "#### **Example 3: Incompatible Shapes**  \n",
    "```python\n",
    "Tensor A: [[1, 2],  # Shape (2, 2)\n",
    "           [3, 4]]  \n",
    "Tensor B: [1, 2, 3]  # Shape (3) → ERROR!\n",
    "```\n",
    "**Why?**:  \n",
    "```\n",
    "Mismatch at dim=1: \n",
    "A's dim=1 is 2, B's dim=0 is 3 → Not broadcastable!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Broadcasting Rules Diagram**  \n",
    "```\n",
    "Step 1: Align shapes right-to-left\n",
    "        A (4, 3, 2)\n",
    "        B    (3, 1)   ← Pad missing dims with 1 → (1, 3, 1)\n",
    "\n",
    "Step 2: Compare dimensions:\n",
    "        Dim 2: A=2, B=1 → B stretches to 2\n",
    "        Dim 1: A=3, B=3 → Match\n",
    "        Dim 0: A=4, B=1 → B stretches to 4\n",
    "\n",
    "Final Shape: (4, 3, 2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **When Broadcasting Fails**  \n",
    "```\n",
    "Tensor A: (3, 4, 5)  \n",
    "Tensor B:    (2, 5)  \n",
    "\n",
    "Error: Dim 1 is 4 (A) vs 2 (B) → Neither is 1!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Key Takeaways**  \n",
    "1. **Right-to-Left**: PyTorch compares shapes from the last dimension backward.  \n",
    "2. **Stretch 1s**: Dimensions of size 1 are copied to match larger tensors.  \n",
    "3. **No Magic**: If dimensions aren’t 1 or equal, broadcasting fails.  \n",
    "\n",
    "\n",
    "\n",
    "**Visual Mnemonic**:  \n",
    "Imagine \"expanding\" the smaller tensor like a rubber band to fit the larger one’s shape, but only if the dimensions are compatible!\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc8170",
   "metadata": {},
   "source": [
    "### **1.6 Memory Management in PyTorch**  \n",
    "*(Views vs. Copies, GPU/CPU Transfers, and Optimization Techniques)*  \n",
    "\n",
    "\n",
    "## **1. Views vs. Copies**  \n",
    "PyTorch tensors can share storage (views) or create independent copies (clones).  \n",
    "\n",
    "| Operation | Memory Shared? | Syntax | Use Case |\n",
    "|-----------|----------------|--------|----------|\n",
    "| **View** | ✅ Yes | `.view()`, `.reshape()`, `.t()` | Fast shape changes |\n",
    "| **Shallow Copy** | ✅ Yes | `a = b` | Reference assignment |\n",
    "| **Clone** | ❌ No | `.clone()` | Isolate tensor for modification |\n",
    "| **Contiguous Copy** | ❌ No | `.contiguous()` | Fix non-contiguous layouts |\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# View (shares memory)\n",
    "b = a.view(4)       # b is [1, 2, 3, 4]\n",
    "a[0, 0] = 99       # b also becomes [99, 2, 3, 4]\n",
    "\n",
    "# Clone (independent copy)\n",
    "c = a.clone()       # c gets new memory\n",
    "a[0, 0] = 0        # c remains unchanged\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **2. Checking Memory Properties**  \n",
    "\n",
    "| Method | Purpose | Example |\n",
    "|--------|---------|---------|\n",
    "| `.is_contiguous()` | Checks memory layout | `a.is_contiguous()` |\n",
    "| `.storage().data_ptr()` | Memory address | `a.storage().data_ptr() == b.storage().data_ptr()` |\n",
    "| `id()` | Python object ID | `id(a) == id(b)` |\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "a = torch.rand(3, 3)\n",
    "b = a.t()  # Transpose (non-contiguous)\n",
    "\n",
    "print(a.is_contiguous())  # True  \n",
    "print(b.is_contiguous())  # False  \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **3. GPU/CPU Transfers**  \n",
    "\n",
    "| Operation | Syntax | Notes |\n",
    "|-----------|--------|-------|\n",
    "| **Move to GPU** | `.to('cuda')` | Requires CUDA GPU |\n",
    "| **Move to CPU** | `.to('cpu')` | Default device |\n",
    "| **Pin Memory** | `.pin_memory()` | Faster GPU transfers (for DataLoader) |\n",
    "\n",
    "**Example**:  \n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Transfer tensor\n",
    "x = torch.rand(3, 3).to(device)  \n",
    "\n",
    "# Pin memory (for DataLoader)\n",
    "y = torch.rand(100, 3).pin_memory()  # Used with CUDA\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **4. Memory Optimization Techniques**  \n",
    "\n",
    "### **A. Avoid Unnecessary Copies**  \n",
    "```python\n",
    "# Bad: Creates temporary copy\n",
    "x = x.contiguous().view(-1)  \n",
    "\n",
    "# Good: Use reshape() which handles non-contiguous tensors\n",
    "x = x.reshape(-1)  \n",
    "```\n",
    "\n",
    "### **B. Reuse Memory with `torch.no_grad()`**  \n",
    "```python\n",
    "with torch.no_grad():  # Disables gradient tracking\n",
    "    y = model(x)       # Reduces memory overhead\n",
    "```\n",
    "\n",
    "### **C. Gradient Clearing**  \n",
    "```python\n",
    "optimizer.zero_grad()  # Clear old gradients to free memory\n",
    "```\n",
    "\n",
    "### **D. Mixed Precision Training**  \n",
    "```python\n",
    "scaler = torch.cuda.amp.GradScaler()  # Reduces GPU memory usage\n",
    "with torch.cuda.amp.autocast():\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **5. Common Pitfalls & Fixes**  \n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| **\"CUDA out of memory\"** | Reduce batch size, use `.empty_cache()` |\n",
    "| **Non-contiguous errors** | Call `.contiguous()` before operations |\n",
    "| **Accidental view modifications** | Use `.clone()` for independent tensors |\n",
    "| **Slow DataLoader** | Enable `pin_memory=True` |\n",
    "\n",
    "**Example (Free GPU Memory)**:  \n",
    "```python\n",
    "torch.cuda.empty_cache()  # Releases unused GPU memory\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **6. Visual Guide: Memory Sharing**  \n",
    "\n",
    "```\n",
    "Original Tensor (a): [1, 2, 3, 4]  \n",
    "    │\n",
    "    ├─ View (b = a.view(2, 2)): Shares memory  \n",
    "    │   └─ Modifying b affects a  \n",
    "    │\n",
    "    └─ Clone (c = a.clone()): New memory  \n",
    "        └─ Independent of a\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **7. Summary Cheatsheet**  \n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| **Create view** | `b = a.view(...)` or `a.reshape(...)` |\n",
    "| **Force copy** | `b = a.clone()` |\n",
    "| **Fix non-contiguous** | `a = a.contiguous()` |\n",
    "| **Move to GPU** | `a = a.to('cuda')` |\n",
    "| **Free GPU memory** | `torch.cuda.empty_cache()` |\n",
    "\n",
    "\n",
    "### **Deep Dive: PyTorch Memory Management Mastery**  \n",
    "*(Advanced optimization techniques, low-level control, and real-world GPU/CPU workflows)*  \n",
    "\n",
    "\n",
    "\n",
    "#### **1. Memory Layout: Strides & Contiguity**  \n",
    "PyTorch tensors use **strides** to map indices to memory locations. Understanding this is key for performance.\n",
    "\n",
    "```python\n",
    "x = torch.rand(3, 4)  \n",
    "print(x.stride())  # (4, 1) ← Elements in dim=0 are 4 apart in memory\n",
    "\n",
    "# Transpose breaks contiguity\n",
    "y = x.t()         # Stride becomes (1, 4)\n",
    "print(y.is_contiguous())  # False\n",
    "\n",
    "# Fix with contiguous() (copies memory)\n",
    "z = y.contiguous()  # Stride (3, 1)\n",
    "```\n",
    "\n",
    "**When to care?**  \n",
    "- Before operations like `view()` that require contiguous tensors  \n",
    "- When passing data to C/CUDA extensions  \n",
    "\n",
    "\n",
    "\n",
    "#### **2. Advanced View Operations**  \n",
    "Beyond `view()`/`reshape()`, PyTorch offers precise control:\n",
    "\n",
    "```python\n",
    "x = torch.rand(2, 3, 4)\n",
    "\n",
    "# Unfold (sliding window)\n",
    "unfolded = x.unfold(1, 2, 1)  # Dim=1, size=2, step=1 → shape (2, 2, 4, 2)\n",
    "\n",
    "# As_strided (manual stride control)\n",
    "custom_view = x.as_strided(size=(2, 6), stride=(12, 2))  # Risky but powerful\n",
    "```\n",
    "\n",
    "**Use Case**: Implementing custom kernels without memory copies.\n",
    "\n",
    "\n",
    "\n",
    "#### **3. Zero-Copy Transfers (Advanced GPU Optimization)**  \n",
    "\n",
    "##### **A. Pinned Memory for Async GPU Transfers**  \n",
    "```python\n",
    "# Allocate pinned (page-locked) CPU memory\n",
    "pinned_tensor = torch.rand(1000, 1000).pin_memory()\n",
    "\n",
    "# Async transfer to GPU (faster for DataLoader)\n",
    "with torch.cuda.stream(torch.cuda.Stream()):\n",
    "    gpu_tensor = pinned_tensor.to('cuda', non_blocking=True)\n",
    "```\n",
    "\n",
    "##### **B. Direct GPU-GPU Copies**  \n",
    "```python\n",
    "# Between GPUs (multi-GPU setups)\n",
    "tensor_gpu1 = torch.rand(3, 3, device='cuda:0')\n",
    "tensor_gpu2 = tensor_gpu1.to('cuda:1')  # Implicit copy\n",
    "\n",
    "# Peer-to-peer (faster for frequent transfers)\n",
    "torch.cuda.set_device(0)\n",
    "tensor_gpu2 = tensor_gpu1.cuda(1)  # Uses NVLink if available\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### **4. Memory Profiling & Debugging**  \n",
    "\n",
    "##### **A. Tracking Allocations**  \n",
    "```python\n",
    "# Enable memory snapshot\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "# Your code here\n",
    "x = torch.rand(10000, 10000, device='cuda')\n",
    "\n",
    "# Get snapshot\n",
    "print(torch.cuda.memory._snapshot())  # Shows allocations/frees\n",
    "```\n",
    "\n",
    "##### **B. Finding Memory Leaks**  \n",
    "```python\n",
    "# Wrap code in memory checker\n",
    "with torch.autograd.profiler.profile(profile_memory=True) as prof:\n",
    "    train_model()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\"))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### **5. In-Place Operations (Advanced Use)**  \n",
    "Use with caution (breaks autograd if not careful):\n",
    "\n",
    "```python\n",
    "x = torch.rand(3, 3, requires_grad=True)\n",
    "\n",
    "# Safe in-place (PyTorch-managed)\n",
    "x.add_(1)  # Keeps gradient history\n",
    "\n",
    "# Unsafe in-place (manual)\n",
    "x.data += 1  # Bypasses autograd (risk of incorrect gradients)\n",
    "```\n",
    "\n",
    "**Golden Rule**: Prefer out-of-place ops unless memory-bound.\n",
    "\n",
    "\n",
    "\n",
    "#### **6. Custom Memory Allocators**  \n",
    "For extreme optimization (C++ required):\n",
    "\n",
    "```cpp\n",
    "#include <torch/cuda/caching_host_allocator.h>\n",
    "\n",
    "// Register custom allocator\n",
    "c10::cuda::CUDACachingAllocator::set_allocator_settings(\"expandable_segments:False\");\n",
    "```\n",
    "\n",
    "**Use Case**:  \n",
    "- Real-time systems with strict latency requirements  \n",
    "- Avoiding CUDA fragmentation  \n",
    "\n",
    "\n",
    "\n",
    "#### **7. Real-World Optimization Pipeline**  \n",
    "\n",
    "```python\n",
    "def train_optimized():\n",
    "    # 1. Pin memory for DataLoader\n",
    "    loader = DataLoader(dataset, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    # 2. Mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # 3. Gradient accumulation\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs.cuda(non_blocking=True))\n",
    "            loss = criterion(outputs, targets.cuda(non_blocking=True))\n",
    "        \n",
    "        # 4. Overlap compute/data transfer\n",
    "        next_inputs, next_targets = prefetch_next_batch(loader)\n",
    "        \n",
    "        # 5. Memory-efficient backward\n",
    "        scaler.scale(loss).backward()\n",
    "        if i % 2 == 0:  # Accumulate gradients\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)  # Saves memory\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Pro-Level Cheatsheet**  \n",
    "\n",
    "| Technique | When to Use | Code Snippet |\n",
    "|-----------|-------------|--------------|\n",
    "| **Pinned Memory** | High-speed data loading | `tensor.pin_memory()` |\n",
    "| **Non-blocking Xfer** | Hide PCIe latency | `.to(device, non_blocking=True)` |\n",
    "| **Gradient Checkpointing** | Memory-heavy models | `torch.utils.checkpoint.checkpoint` |\n",
    "| **TensorRT Integration** | Production deployment | `torch2trt` |\n",
    "| **Custom CUDA Kernels** | Ultimate performance | `torch.cuda.jit.script` |\n",
    "\n",
    "\n",
    "\n",
    "### **Debugging Memory Issues**  \n",
    "1. **OOM Errors**:  \n",
    "   ```python\n",
    "   torch.cuda.empty_cache()  # Temporary fix\n",
    "   ```\n",
    "2. **Fragmentation**:  \n",
    "   ```python\n",
    "   torch.cuda.reset_peak_memory_stats()  # Track true usage\n",
    "   ```\n",
    "3. **Memory Snapshot**:  \n",
    "   ```python\n",
    "   from torch.cuda import memory_summary\n",
    "   print(memory_summary())\n",
    "   ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62110444",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
